{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.951785695971921, 0.669569988269359, 0.170130254933611, 0.623469153884798, 0.925886165117845, 0.81268464284949], [0.51044699898921, 0.922627209918573, 0.0878990164492279, 0.0254153392743319, 0.698443632107228, 0.658544838894159]]\n",
      "['green']\n",
      "['white']\n",
      "[3.70751388051659]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data_r=[]\n",
    "data_c=[]\n",
    "data_h=[]\n",
    "label_list=[]\n",
    "with open('intern_data.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        #print(row['a'], row['b'],row['c'],row['d'])\n",
    "        \n",
    "        data_r.append([float(row['a']), float(row['b']),float(row['d']),float(row['e']),float(row['f']),float(row['g'])])\n",
    "        data_c.append(row['c'])\n",
    "        data_h.append(row['h'])\n",
    "        label_list.append(float(row['y']))\n",
    "print(data_r[:2])\n",
    "print(data_c[:1])\n",
    "print(data_h[:1])\n",
    "print(label_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Hot encoding C attribute\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['green' 'red' 'yellow' 'green' 'red' 'blue' 'yellow' 'green' 'green'\n",
      " 'green']\n",
      "[1 2 3 1 2 0 3 1 1 1]\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hot encoding H attribute\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['white' 'white' 'black' 'black' 'white' 'white' 'white' 'white' 'white'\n",
      " 'white']\n",
      "[1 1 0 0 1 1 1 1 1 1]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "values_c = array(data_c)\n",
    "print('-'*100)\n",
    "print('Hot encoding C attribute')\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "print(values_c[:10])\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_c = label_encoder.fit_transform(values_c)\n",
    "print(integer_encoded_c[:10])\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "#chaging the shape to a single column from row\n",
    "integer_encoded_c = integer_encoded.reshape(len(integer_encoded_c), 1)\n",
    "\n",
    "onehot_encoded_c = onehot_encoder.fit_transform(integer_encoded_c)\n",
    "print(onehot_encoded_c[:10])\n",
    "#inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "values_h = array(data_h)\n",
    "print('-'*100)\n",
    "print('Hot encoding H attribute')\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "print(values_h[:10])\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_h = label_encoder.fit_transform(values_h)\n",
    "print(integer_encoded_h[:10])\n",
    "# binary encode\n",
    "\n",
    "#chaging the shape to a single column from row\n",
    "integer_encoded_h = integer_encoded_h.reshape(len(integer_encoded_h), 1)\n",
    "onehot_encoded_h = onehot_encoder.fit_transform(integer_encoded_h)\n",
    "print(onehot_encoded_h[:10])\n",
    "#inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)\n",
    "\n",
    "print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[[ 0.9517857   0.66956999  0.17013025  0.62346915  0.92588617  0.81268464\n",
      "   0.          1.          0.          0.          0.          1.        ]\n",
      " [ 0.510447    0.92262721  0.08789902  0.02541534  0.69844363  0.65854484\n",
      "   0.          0.          1.          0.          0.          1.        ]]\n",
      "Label:\n",
      "[ 3.70751388  2.68924256]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_arr=np.array(data_r)\n",
    "#print(data_arr[:2])\n",
    "\n",
    "data = np.concatenate((data_arr,onehot_encoded_c, onehot_encoded_h), axis=1)\n",
    "print('Data:')\n",
    "print(data[:2])\n",
    "label=np.array(label_list)\n",
    "print('Label:')\n",
    "print(label[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.00730915  0.39185073  0.00132106  0.95165108  0.67181686  0.60326591\n",
      " -0.58650175  0.31616989 -0.02141574  0.2917476  -0.31682548  0.31682548]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "error = label-pred\n",
      "max: 0.274337165622\n",
      "min: -0.324984624686\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(data, label)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred = regr.predict(data)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(label, pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(label, pred))\n",
    "\n",
    "print('-'*100)\n",
    "print('error = label-pred')\n",
    "error= label-pred\n",
    "print('max:',error.max())\n",
    "print('min:',error.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.0071977   0.39082084  0.00126427  0.94890552  0.67007086  0.60196517\n",
      " -0.58591719  0.31587259 -0.02151798  0.29156258 -0.31669966  0.31669966]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "error = label-pred\n",
      "max: 0.272378207022\n",
      "min: -0.324424425406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr = Ridge(alpha=0.1)\n",
    "# Train the model using the training sets\n",
    "regr.fit(data, label)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred = regr.predict(data)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(label, pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(label, pred))\n",
    "\n",
    "print('-'*100)\n",
    "print('error = label-pred')\n",
    "error= label-pred\n",
    "print('max:',error.max())\n",
    "print('min:',error.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.35\n",
      "Variance score: 0.00\n",
      "error = label-pred\n",
      "max: 1.29770659078\n",
      "min: -1.70142661735\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "error = label-pred\n",
      "max: 0.272378207022\n",
      "min: -0.324424425406\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "error = label-pred\n",
      "max: 0.219941906371\n",
      "min: -0.317582420906\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.98\n",
      "error = label-pred\n",
      "max: 0.221865787889\n",
      "min: -0.240969147245\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.00\n",
      "Variance score: 0.99\n",
      "error = label-pred\n",
      "max: 0.224163444038\n",
      "min: -0.228753009372\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.00\n",
      "Variance score: 0.99\n",
      "error = label-pred\n",
      "max: 0.204065815363\n",
      "min: -0.194461477661\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.00\n",
      "Variance score: 1.00\n",
      "error = label-pred\n",
      "max: 0.179140301781\n",
      "min: -0.16898720733\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mean squared error: 0.00\n",
      "Variance score: 1.00\n",
      "error = label-pred\n",
      "max: 0.155082225619\n",
      "min: -0.141799618931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "for degree in [0,1,2, 3, 4, 5,6,7]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree,include_bias=True), Ridge(alpha=0.1))\n",
    "    model.fit(data, label)\n",
    "    pred = model.predict(data)\n",
    "    # The mean squared error\n",
    "    print('-'*200)\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(label, pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(label, pred))\n",
    "    print('error = label-pred')\n",
    "    error= label-pred\n",
    "    print('max:',error.max())\n",
    "    print('min:',error.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression: with Test DataSet and CV\n",
    "### Cross Validation on training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 1\n",
      "Scores: [ 0.96934026  0.96531253  0.96973964  0.97079496]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.90649916  2.95732474  2.75024203  2.07445605  3.22967906]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 2\n",
      "Scores: [ 0.96320491  0.95784519  0.96126743  0.95571397]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.95985861  2.94427767  2.75723716  2.12278776  3.17373051]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 3\n",
      "Scores: [ 0.95858958  0.95582071  0.95818016  0.94854812]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.9519472   2.95925912  2.74326533  2.09518673  3.21752173]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 4\n",
      "Scores: [ 0.95303342  0.95064701  0.94981517  0.94000957]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.89882526  3.05963365  2.71891413  2.00784082  3.2773074 ]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 5\n",
      "Scores: [ 0.9543531   0.95038635  0.94832042  0.93089327]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.96\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.93145485  3.03630926  2.73468396  2.0631827   3.20095165]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV \n",
    "\n",
    "# 30% of the data for the Test is kept at random\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for degree in [1, 2, 3, 4, 5]:\n",
    "    print('-'*200)\n",
    "    print('degree:',degree)\n",
    "    model = make_pipeline(PolynomialFeatures(degree,include_bias=True), RidgeCV(alphas=[0.001,0.01,0.1,1,10,100],fit_intercept=True,cv=4))\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=4)\n",
    "    print('Scores:',scores)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, pred))\n",
    "    print('y_test:',y_test[:5])\n",
    "    print('pred:  ',pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 1\n",
      "Scores: [ 0.9682832   0.96882629  0.96765279  0.97283368]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.89633672  2.93799101  2.7411413   2.07556002  3.22769496]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 2\n",
      "Scores: [ 0.96752401  0.96364198  0.9633273   0.96972415]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.90235429  2.93821114  2.73435303  2.08723873  3.22719309]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 3\n",
      "Scores: [ 0.96736743  0.96371446  0.96306529  0.96814192]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.89891649  2.93018998  2.73657945  2.08121764  3.23092439]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 4\n",
      "Scores: [ 0.96724436  0.963208    0.96309492  0.96789818]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.89883528  2.93098195  2.73646399  2.08064732  3.23068177]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "degree: 5\n",
      "Scores: [ 0.96714576  0.96295213  0.96315074  0.96783945]\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.97\n",
      "y_test: [ 2.85508529  2.94973971  2.75635373  1.96371667  3.3201863 ]\n",
      "pred:   [ 2.90003268  2.93138286  2.73655095  2.08153764  3.22621403]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV \n",
    "\n",
    "# 30% of the data for the Test is kept at random\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for degree in [1, 2, 3, 4, 5]:\n",
    "    print('-'*200)\n",
    "    print('degree:',degree)\n",
    "    model = make_pipeline(PolynomialFeatures(degree,include_bias=True), LassoCV(alphas=[0.001,0.01,0.1,1,10,100],fit_intercept=True,cv=4))\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=4)\n",
    "    print('Scores:',scores)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, pred))\n",
    "    print('y_test:',y_test[:5])\n",
    "    print('pred:  ',pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR complexity and bandwidth selected and model fitted in 1.021 s\n",
      "Mean squared error: 0.01\n",
      "Variance score: 0.9724\n",
      "Sample y_test: [ 3.44333169  1.55023629  3.43498205  2.17696001  2.83458413]\n",
      "Sample pred:   [ 3.3780122   1.4180844   3.58964151  2.18511231  2.86856006]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Random sample of the data for the Test is kept\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.25, random_state=2)\n",
    "\n",
    "\n",
    "kr = GridSearchCV(KernelRidge(kernel='rbf'), cv=5,\n",
    "                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                              \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "t0 = time.time()\n",
    "kr.fit(X_train, y_train)\n",
    "kr_fit = time.time() - t0\n",
    "print(\"KRR complexity and bandwidth selected and model fitted in %.3f s\"\n",
    "      % kr_fit)\n",
    "\n",
    "\n",
    "pred = kr.predict(X_test)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.4f' % r2_score(y_test, pred))\n",
    "print('Sample y_test:',y_test[:5])\n",
    "print('Sample pred:  ',pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
